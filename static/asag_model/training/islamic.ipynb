{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "islamic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "auKejlyq2Z-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d0de040-5348-4f47-921a-044bd876517f"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from keras import utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXw_hNJpbh4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c8e63980-7e61-405d-d626-f67f85892917"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('drive/My Drive/Data/wiki.ar.vec')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt4ri_7vV3WQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5e728536-988c-4bcf-d56d-fefd16048c3c"
      },
      "source": [
        "# read data \n",
        "data_path = '/content/drive/My Drive/'\n",
        "stu_answers= pd.read_csv(os.path.join(data_path, 'stu-answers.csv'), encoding='utf-8')\n",
        "stu_answers = stu_answers.replace(np.nan, '', regex=True)\n",
        "stu_answers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>stu_answer</th>\n",
              "      <th>grade</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18</td>\n",
              "      <td>ز</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>جبرائيل</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18</td>\n",
              "      <td>جبريل</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>جبريل عليه السلام</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>سيدنا جبريل عليه السلام</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1255</th>\n",
              "      <td>1</td>\n",
              "      <td>الجنه</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1256</th>\n",
              "      <td>1</td>\n",
              "      <td>جزاءه الجنة و رضى الله</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257</th>\n",
              "      <td>1</td>\n",
              "      <td>‏كما قال الرسول صلى الله عليه وسلم : \" صبرا آل...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>1</td>\n",
              "      <td>جزاءه الجنة</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1259</th>\n",
              "      <td>1</td>\n",
              "      <td>الجنة</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1260 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      question                                         stu_answer  grade\n",
              "0           18                                                  ز      0\n",
              "1           18                                            جبرائيل      1\n",
              "2           18                                              جبريل      2\n",
              "3           18                                  جبريل عليه السلام      2\n",
              "4           18                            سيدنا جبريل عليه السلام      2\n",
              "...        ...                                                ...    ...\n",
              "1255         1                                              الجنه      2\n",
              "1256         1                             جزاءه الجنة و رضى الله      2\n",
              "1257         1  ‏كما قال الرسول صلى الله عليه وسلم : \" صبرا آل...      2\n",
              "1258         1                                        جزاءه الجنة      2\n",
              "1259         1                                              الجنة      2\n",
              "\n",
              "[1260 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L94z9epwb1up",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "738035d5-431b-44b5-cc5e-c7f46d03d03a"
      },
      "source": [
        "# preprocessing \n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# stop words\n",
        "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.arlstem import ARLSTem\n",
        "stemmmer = ARLSTem()\n",
        "\n",
        "def remove_stowords(elements):\n",
        "  corps = []\n",
        "  for string in elements :\n",
        "    string = string.strip()\n",
        "    string = string.split()\n",
        "    string = [ stemmmer.stem(word) for word in string if not word in arb_stopwords ]\n",
        "    string = ' '.join(string)\n",
        "    corps.append(string)\n",
        "  return corps"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcHaZhToX2eL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4d09d15-783f-4da1-9578-2e253c031408"
      },
      "source": [
        "answers = stu_answers['stu_answer'].tolist()\n",
        "scores = stu_answers['grade'].tolist()\n",
        "scores = utils.to_categorical(scores)\n",
        "corps = remove_stowords(answers)\n",
        "scores.shape,len(corps)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1260, 3), 1260)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at2O1RYZYrir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "7aaabde8-dfd4-4b64-9537-8031549ecdbd"
      },
      "source": [
        "fasttext_model.most_similar('جبريل')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('بجبريل', 0.8251434564590454),\n",
              " ('وجبريل', 0.7967433333396912),\n",
              " ('جبريل،', 0.7817331552505493),\n",
              " ('جبريلُ', 0.7257490158081055),\n",
              " ('لجبريل', 0.7167991399765015),\n",
              " ('جبرئيل', 0.5911530256271362),\n",
              " ('الرجوب،', 0.5503920912742615),\n",
              " ('فسجد', 0.5416853427886963),\n",
              " ('الرجوب', 0.535503625869751),\n",
              " ('بالنبي', 0.5288292169570923)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4FkXY4Zb2eR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a25a1b6-319e-40d8-cc92-ff79f4ebb643"
      },
      "source": [
        "\n",
        "# tokenization\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence , one_hot , text_to_word_sequence\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(filters=''''!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''''' )\n",
        "tokenizer.fit_on_texts(corps)\n",
        "sequences = tokenizer.texts_to_sequences(corps)\n",
        "max_sequence_length = max(len(s) for s in sequences)\n",
        "sequences = pad_sequences(sequences,max_sequence_length)\n",
        "word2idx = tokenizer.word_index\n",
        "vocab_size = len(word2idx) + 1\n",
        "\n",
        "# word embedding\n",
        "from keras.layers import Embedding\n",
        "import numpy as np\n",
        "EMBEDDING_DIM = 300\n",
        "num_words = len(word2idx) + 1\n",
        "# prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, idx in word2idx.items():\n",
        "    if (word in fasttext_model) :\n",
        "        embedding_matrix[idx] = fasttext_model.get_vector(word)\n",
        "    else :\n",
        "      #embedding_matrix[idx] = fasttext_model.get_vector(\"unk\")\n",
        "      print(\"  word not exist in voca ---> \" + word)    \n",
        "\n",
        "\n",
        "# load pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  word not exist in voca ---> جهري\n",
            "  word not exist in voca ---> فاصدع\n",
            "  word not exist in voca ---> 3\n",
            "  word not exist in voca ---> 13\n",
            "  word not exist in voca ---> ؤمر\n",
            "  word not exist in voca ---> تعديب\n",
            "  word not exist in voca ---> جزاؤ\n",
            "  word not exist in voca ---> اهنة\n",
            "  word not exist in voca ---> يذء\n",
            "  word not exist in voca ---> رضو\n",
            "  word not exist in voca ---> بالل\n",
            "  word not exist in voca ---> 13سن\n",
            "  word not exist in voca ---> 3سنو\n",
            "  word not exist in voca ---> سخري\n",
            "  word not exist in voca ---> بمتل\n",
            "  word not exist in voca ---> تتك\n",
            "  word not exist in voca ---> بمك\n",
            "  word not exist in voca ---> 3سن\n",
            "  word not exist in voca ---> ندو\n",
            "  word not exist in voca ---> فموعد\n",
            "  word not exist in voca ---> بدعو\n",
            "  word not exist in voca ---> اقربين\n",
            "  word not exist in voca ---> عرضو\n",
            "  word not exist in voca ---> 4\n",
            "  word not exist in voca ---> اثن\n",
            "  word not exist in voca ---> 3افراد\n",
            "  word not exist in voca ---> بجرا\n",
            "  word not exist in voca ---> قتصر\n",
            "  word not exist in voca ---> عاءل\n",
            "  word not exist in voca ---> 10\n",
            "  word not exist in voca ---> سمء\n",
            "  word not exist in voca ---> ستهزء\n",
            "  word not exist in voca ---> ستمرر\n",
            "  word not exist in voca ---> بقداء\n",
            "  word not exist in voca ---> حتساب\n",
            "  word not exist in voca ---> ؤدي\n",
            "  word not exist in voca ---> مضاهر\n",
            "  word not exist in voca ---> 2\n",
            "  word not exist in voca ---> انص\n",
            "  word not exist in voca ---> فانذر\n",
            "  word not exist in voca ---> علنو\n",
            "  word not exist in voca ---> عدبو\n",
            "  word not exist in voca ---> ستشهدو\n",
            "  word not exist in voca ---> 5\n",
            "  word not exist in voca ---> صبرو\n",
            "  word not exist in voca ---> داءم\n",
            "  word not exist in voca ---> ‏اسم\n",
            "  word not exist in voca ---> ‏اول\n",
            "  word not exist in voca ---> بسمي\n",
            "  word not exist in voca ---> 13سنة\n",
            "  word not exist in voca ---> رسولص\n",
            "  word not exist in voca ---> 12سن\n",
            "  word not exist in voca ---> لاتةعشر\n",
            "  word not exist in voca ---> بكو\n",
            "  word not exist in voca ---> 12\n",
            "  word not exist in voca ---> فدخلو\n",
            "  word not exist in voca ---> ١٣سنة\n",
            "  word not exist in voca ---> توحيدالله\n",
            "  word not exist in voca ---> بانذار\n",
            "  word not exist in voca ---> متاز\n",
            "  word not exist in voca ---> اﻷقرب\n",
            "  word not exist in voca ---> ٣سنو\n",
            "  word not exist in voca ---> دامت3سنو\n",
            "  word not exist in voca ---> بدعوةاهل\n",
            "  word not exist in voca ---> ‏دام\n",
            "  word not exist in voca ---> رعمر\n",
            "  word not exist in voca ---> عاءشةرضي\n",
            "  word not exist in voca ---> عنهل\n",
            "  word not exist in voca ---> خديجةرضي\n",
            "  word not exist in voca ---> حرثة\n",
            "  word not exist in voca ---> ستوط\n",
            "  word not exist in voca ---> خري\n",
            "  word not exist in voca ---> كترث\n",
            "  word not exist in voca ---> ‏تحمل\n",
            "  word not exist in voca ---> ختشء\n",
            "  word not exist in voca ---> بقصاء\n",
            "  word not exist in voca ---> هداي\n",
            "  word not exist in voca ---> يدء\n",
            "  word not exist in voca ---> مختشي\n",
            "  word not exist in voca ---> اردة\n",
            "  word not exist in voca ---> تهام\n",
            "  word not exist in voca ---> حينم\n",
            "  word not exist in voca ---> صرو\n",
            "  word not exist in voca ---> باطلهم\n",
            "  word not exist in voca ---> ستغلل\n",
            "  word not exist in voca ---> مواصلةالدعو\n",
            "  word not exist in voca ---> مقابلةالسب\n",
            "  word not exist in voca ---> بمتله\n",
            "  word not exist in voca ---> عوة\n",
            "  word not exist in voca ---> بترفع\n",
            "  word not exist in voca ---> 1\n",
            "  word not exist in voca ---> ‏المرحل\n",
            "  word not exist in voca ---> سغء\n",
            "  word not exist in voca ---> متمني\n",
            "  word not exist in voca ---> يذئ\n",
            "  word not exist in voca ---> 0\n",
            "  word not exist in voca ---> ستمع\n",
            "  word not exist in voca ---> ‏اقبل\n",
            "  word not exist in voca ---> يشرم\n",
            "  word not exist in voca ---> كدال\n",
            "  word not exist in voca ---> فابدع\n",
            "  word not exist in voca ---> مبي\n",
            "  word not exist in voca ---> اطع\n",
            "  word not exist in voca ---> متق\n",
            "  word not exist in voca ---> حكمةوالموعظ\n",
            "  word not exist in voca ---> موعض\n",
            "  word not exist in voca ---> تلمشرك\n",
            "  word not exist in voca ---> فاجهر\n",
            "  word not exist in voca ---> فاثدع\n",
            "  word not exist in voca ---> اعر\n",
            "  word not exist in voca ---> 《فصدع\n",
            "  word not exist in voca ---> تومن\n",
            "  word not exist in voca ---> سالح\n",
            "  word not exist in voca ---> سؤززسزس\n",
            "  word not exist in voca ---> اوءل\n",
            "  word not exist in voca ---> عتنق\n",
            "  word not exist in voca ---> اسرابو\n",
            "  word not exist in voca ---> محزوم\n",
            "  word not exist in voca ---> سمس\n",
            "  word not exist in voca ---> فيعذب\n",
            "  word not exist in voca ---> عديب\n",
            "  word not exist in voca ---> ضعو\n",
            "  word not exist in voca ---> اشع\n",
            "  word not exist in voca ---> حرقو\n",
            "  word not exist in voca ---> سزسم\n",
            "  word not exist in voca ---> عمارب\n",
            "  word not exist in voca ---> 3هم\n",
            "  word not exist in voca ---> سميةبن\n",
            "  word not exist in voca ---> ثلاثةافراد\n",
            "  word not exist in voca ---> 3اشخاص\n",
            "  word not exist in voca ---> ‏الاب\n",
            "  word not exist in voca ---> ‫ياسر\n",
            "  word not exist in voca ---> زسنس\n",
            "  word not exist in voca ---> فاؤل\n",
            "  word not exist in voca ---> تباث\n",
            "  word not exist in voca ---> ‏نصح\n",
            "  word not exist in voca ---> سظس\n",
            "  word not exist in voca ---> جنةصبرا\n",
            "  word not exist in voca ---> بدجول\n",
            "  word not exist in voca ---> ‏بشر\n",
            "  word not exist in voca ---> االل\n",
            "  word not exist in voca ---> سزس\n",
            "  word not exist in voca ---> مطمئ\n",
            "  word not exist in voca ---> بينم\n",
            "  word not exist in voca ---> يتعدب\n",
            "  word not exist in voca ---> قولالرسول\n",
            "  word not exist in voca ---> ثب\n",
            "  word not exist in voca ---> ضحو\n",
            "  word not exist in voca ---> نتثب\n",
            "  word not exist in voca ---> بايم\n",
            "  word not exist in voca ---> بمثلهوغضب\n",
            "  word not exist in voca ---> عذبو\n",
            "  word not exist in voca ---> 《احب\n",
            "  word not exist in voca ---> ‏ان\n",
            "  word not exist in voca ---> كافؤ\n",
            "  word not exist in voca ---> فجزائ\n",
            "  word not exist in voca ---> بتلء\n",
            "  word not exist in voca ---> تظحي\n",
            "  word not exist in voca ---> جنةهو\n",
            "  word not exist in voca ---> لجن\n",
            "  word not exist in voca ---> عضيم\n",
            "  word not exist in voca ---> ‏كم\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSmP-meab5Uy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9e8a28ad-650e-4531-8ba5-df5c13101cc9"
      },
      "source": [
        "\n",
        "# train model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "#model.add(Embedding(vocab_size,50))\n",
        "model.add(LSTM(16, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(sequences, scores,\n",
        "          batch_size=1, epochs=100)                           \n",
        "model.save('islamic_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1260/1260 [==============================] - 19s 15ms/step - loss: 0.8638 - accuracy: 0.5921\n",
            "Epoch 2/100\n",
            "1260/1260 [==============================] - 19s 15ms/step - loss: 0.6624 - accuracy: 0.7357\n",
            "Epoch 3/100\n",
            "1230/1260 [============================>.] - ETA: 0s - loss: 0.5526 - accuracy: 0.7740"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxIh7eTob8jX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "bd3af8d1-b5c9-487c-d982-e28bc50dc996"
      },
      "source": [
        "\n",
        "def preprocces_input(input_ans):\n",
        "  input_ans = remove_stowords(input_ans)\n",
        "  input_ans = tokenizer.texts_to_sequences(input_ans)\n",
        "  input_seq= pad_sequences(input_ans, maxlen=max_sequence_length)\n",
        "  return input_seq\n",
        "\n",
        "def predict(input_ans) :\n",
        "  input_ans = [input_ans]\n",
        "  input_ans = preprocces_input(input_ans)\n",
        "  pred = model.predict_classes(input_ans)\n",
        "  return pred[0]\n",
        "\n",
        "for ans in stu_answers['stu_answer'].tolist() :\n",
        "  print(ans, predict(input_ans))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-8686192474ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstu_answers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stu_answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-8686192474ae>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(input_ans)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0minput_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0minput_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocces_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-8686192474ae>\u001b[0m in \u001b[0;36mpreprocces_input\u001b[0;34m(input_ans)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocces_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0minput_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stowords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0minput_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0minput_seq\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-478f945928db>\u001b[0m in \u001b[0;36mremove_stowords\u001b[0;34m(elements)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcorps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melements\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mstemmmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mar_stopwords\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'strip'"
          ]
        }
      ]
    }
  ]
}